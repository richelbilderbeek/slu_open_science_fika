# slu_open_science_fika

Preparation for the Open Science at SLU on 2023-10-25.

 * [View the proposal (.pdf)](Proposal-National-guidelines-for-open-science.pdf)

## Abbreviations

 * KB: Kungligt Biblioteket

## Response to letter

## Abstract

I think the proposal by KB
is a step in the right direction,
unconvincing to any critical reader, 
and has no effect on the most prevalent problems.
I will clarify my points here, in the hope to convince KB to make some changes.
However, I can imagine there is an unpublished strategy behind this proposal
and I will definitely give KB the benefit of the doubt in being 
more strategic than I am.
Whatever KB chooses to do, I will applaud their push for more Open Science.

## A step in the right direction

I think the proposal by KB
is a step in the right reaction: to me, 
Open Science is a pleonasm, so making science more Open
is making science more what it should be.
Such a transition takes time and multiple steps.
The six focus area in the proposal are simply that:
the next steps. The next steps, after which there will be more.
This is similar to the advice given to researchers 
that just discovered Open Science:
do one step at a time and gradually build up the Openness
of your research.
I agree that the steps suggested by KB are steps in the right direction,
I can (and will) challenge that it is steps that should be first. 

## Unconvincing to any critical reader

I think the proposal by KB is unconvincing to any critical reader,
even though there is a rationale given for each of the six areas.
The reason the rationales are unconvincing, is because a critical reader
will ask for a reference to the literature, as well as an effect size.

To illustrate with an example, 
I will use the first line 
of the section 'Rationale for open research methods', which reads:
'Open research methods enhance the quality of research through increased reproducibility'.
No reference. On which research do the authors base this statement on?
Assuming the statement is true, the next question will be:
how much is the quality of the research increased?
And how much higher is the quality of the research?
This helps determine the reader if this focus area is worth it.

One reference relevant for this example 
could be [Soderberg et al., 2021], where reviewers
were asked to grade papers for multiple criterial 
on a -4 ('very bad') to 4 ('very good') Rickert scale.
These papers were either 'regular' papers or registered reports (more on those
in the next section). It was found that registered reports
scored better on all criteria. On such criteria is 'Method quality',
where 'regular' papers score 0.29 on average, where registered reports scored.
1.05. Rescaling these values from a [-4, 4] to a [0, 8] scale 
(by simply adding 4.0), results in values 4.29 and 5.05 respectively.
From that one can conclude that registered reports have an 18% higher method quality.
If that percentage is worth it, is still an open question.

Also the bigger picture lacks references and effect sizes, 
underselling the importance of Open Science.
For example, section 1 of the proposal claims
'Open science lays the foundation for future
research through [...] reproducible research methods [...]'.
A critical reader will wonder how reproducible 'regular' research methods are,
to determine this indeed is a problem.
One example is a study that found that 50% of scientists 
cannot reproduce their own work [Baker, 2016].
A second example is about papers in in pre-clinical cancer research, 
of which only 11% of the findings could be replicated [Begley and Ellis, 2012].

Note that it will be very easy to add more references to papers
dealing with the reproducibility of research, by simply using
the search term 'Replication crisis'.

## No effect on the most prevalent problems

I state that the KB proposal has no effect on the most prevalent
problems.
A study on questionable research practices found that the
most prevalent questionable research practices (QRPs) are listed
below (from [Gopalakrishna et al., 2021], the percentage indicates the
percentage of scientists that did this QRP), 
where only the last point is addressed by the KB proposal:

1. 17.5%: No submission of a valid negative finding
2. 17.0%: Not mentioning flaws in the study
3. 15.0%: Too little supervision
4. 14.7%: Too little attention to proper use of technology
5. 14.5%: Too little documentation of the scientific process.












## References


 * [Soderberg et al., 2021] Soderberg, Courtney K., et al. "Initial evidence of research quality of registered reports compared with the standard publishing model." Nature Human Behaviour 5.8 (2021): 990-997.
 * [Baker, 2016] Baker, Monya. "1,500 scientists lift the lid on reproducibility." Nature News 533.7604 (2016): 452.
 * [Begley and Ellis, 2012] Begley, C. Glenn, and Lee M. Ellis. "Raise standards for preclinical cancer research." Nature 483.7391 (2012): 531-533.
 * [Gopalakrishna et al., 2021] Gopalakrishna, Gowri, et al. "Prevalence of questionable research practices, research misconduct and their potential explanatory factors: a survey among academic researchers in The Netherlands." (2021).



[3] Ioannidis, John PA. "Why most published research findings are false." PLoS medicine 2.8 (2005): e124.
[4] Wicherts, Jelte M., et al. "Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid p-hacking." Frontiers in psychology 7 (2016): 1832.
[5] Simmons, Joseph P., Leif D. Nelson, and Uri Simonsohn. "False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant." Psychological science 22.11 (2011): 1359-1366.
